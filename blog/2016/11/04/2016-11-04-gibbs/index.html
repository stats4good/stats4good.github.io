<!DOCTYPE html>
<html lang="pt-br">

  <head>
  <meta charset="utf-8">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Gibbs Sampler Colapsado</title>
  <meta name="author" content="" />

  
  <meta name="keywords" content="S4G, R, Data Science">
  

  

  <meta name="generator" content="Hugo 0.36.1" />

  <link href='//fonts.googleapis.com/css?family=Roboto:400,100,100italic,300,300italic,500,700,800' rel='stylesheet' type='text/css'>

  
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

  
  <link href="/css/animate.css" rel="stylesheet">

  
  
    <link href="/css/style.default.css" rel="stylesheet" id="theme-stylesheet">
  


  
  <link href="/css/custom.css" rel="stylesheet">

  
  
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
    <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" href="/img/apple-touch-icon.png" />
  

  <link href="/css/owl.carousel.css" rel="stylesheet">
  <link href="/css/owl.theme.css" rel="stylesheet">

  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Stats4Good">

  
  <meta property="og:title" content="Gibbs Sampler Colapsado" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="/blog/2016/11/04/2016-11-04-gibbs//" />
  <meta property="og:image" content="img/logo.png" />

</head>

  <body>

    <div id="all">

        <header>

          <div class="navbar-affixed-top" data-spy="affix" data-offset-top="200">

    <div class="navbar navbar-default yamm" role="navigation" id="navbar">

        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand home" href="/">
                    <img src="/img/logo.png" alt="Gibbs Sampler Colapsado logo" class="hidden-xs hidden-sm">
                    <img src="/img/logo-small.png" alt="Gibbs Sampler Colapsado logo" class="visible-xs visible-sm">
                    <span class="sr-only">Gibbs Sampler Colapsado - </span>
                </a>
                <div class="navbar-buttons">
                    <button type="button" class="navbar-toggle btn-template-main" data-toggle="collapse" data-target="#navigation">
                      <span class="sr-only"></span>
                        <i class="fa fa-align-justify"></i>
                    </button>
                </div>
            </div>
            

            <div class="navbar-collapse collapse" id="navigation">
                <ul class="nav navbar-nav navbar-right">
                  
                  <li class="dropdown">
                    
                    <a href="/">Home</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/blog/">Blog</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/proj/">Projetos</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/integrantes/">Integrantes</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/sobre">Sobre</a>
                    
                  </li>
                  
                </ul>
            </div>
            

            <div class="collapse clearfix" id="search">

                <form class="navbar-form" role="search">
                    <div class="input-group">
                        <input type="text" class="form-control" placeholder="Search">
                        <span class="input-group-btn">

                    <button type="submit" class="btn btn-template-main"><i class="fa fa-search"></i></button>

                </span>
                    </div>
                </form>

            </div>
            

        </div>
    </div>
    

</div>




        </header>

        <div id="heading-breadcrumbs">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <h1>Gibbs Sampler Colapsado</h1>
            </div>
        </div>
    </div>
</div>


        <div id="content">
            <div class="container">

                <div class="row">

                    

                    <div class="col-md-9" id="blog-post">

                        <p class="text-muted text-uppercase mb-small text-right"> <a href="#">Milton Pifano para Stats4Good</a> | 04/11/2016</p>

                        <div id="post-content">
                          <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p><font size="5"> O objetivo desta apresentação é mostrar o algoritmo <i> Gibbs Sampler</i> Colapsado no contexto da técnica LDA (<i> Latent Dirichlet Alocation </i>).</p>
<p>Para isso, as seguintes atividades foram desenvolvidas a partir de bases reais de documentos:</p>
<ul>
<li>
Implementação do modelo LDA (<i> Gibbs Sampler </i> padrão);
</li>
<li>
Implementação do algoritmo <i>Collapsed Gibbs Sampler </i> conforme descrito no artigo de Griffiths and Steyvers (2004).
</ul>
<p></font></p>
<h2>
<b> 1 - Definição do Modelo e Algoritmos </b>
</h2>
<h3>
<b> <u> 1.1 - Especificação do modelo LDA </u> </b>
</h3>
<p><font size="5"> A idéia por trás do LDA é :</p>
<ul>
<li>
Modelar documentos oriundos de múltiplos tópicos;
</li>
<li>
Um <i>tópico</i> é definido como tendo uma distribuição de probabilidade sobre um vocabulário fixo de palavras;
<li>
Assume-se que <i>K</i> tópicos estão associados a uma coleção de documentos; e
</li>
<li>
Cada documento exibe os tópicos com diferentes proporções.
</li>
</ul>
<p>Mais formalmente, o LDA se baseia em :</p>
<ul>
<li>
um modelo de variáveis latentes para os documentos;
</li>
<li>
os dados observados são as palavras de cada documento;
</li>
<li>
a estrutura de tópicos é uma estrutura latente. Dado uma coleção, a ditribuição <i>a posteriori</i> das variáveis latentes determina a decomposição de tópicos da coleção.
</li>
</ul>
<p>A interação entre os documentos observados e a estrutura de tópicos latente se manifesta em um processo probabilístico gerador associado ao LDA, processo aleatório imaginário que se assume ter produzido os dados observados.</p>
<div class="figure">
<img src="/blog/2016-11-04-gibbs_files/figure-html/FiguraGraphLDA.png" alt="Figura 1 : Modelo LDA" width="100%" />
<p class="caption">Figura 1 : Modelo LDA</p>
</div>
<ul>
<li>
<span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span> = hiperparâmetros conhecidos <i>a priori</i>;
<li>
<span class="math inline">\(\theta_d \sim Dir(\alpha)\)</span> = proporção de tópicos por documento
<li>
<span class="math inline">\(\phi_k \sim Dir(\beta)\)</span> = K vetores M-dimensionais com dist. probabilidade sobre as M palavras do dicionário.
<li>
<span class="math inline">\(z_{d,n} \sim Mult(1,\theta_d)\)</span> = atribuição de cada palavra a um tópico
<li>
<span class="math inline">\(w_{d,n} | z_{d,n} \sim Mult(1,\phi_{z_{d,n}})\)</span> = palavra observada
</ul>
<p>O processo pode ser descrito como :</p>
<ul>
<li>
Documento escolhe seus tópicos <span class="math inline">\(\theta_d = (\theta_{d1}, \theta_{d2}, \theta_{d3}, ..., \theta_{dk})\)</span> que são vetores i.i.d com distribuição Dir(<span class="math inline">\(\alpha_1,\alpha_2,\alpha_2,...,\alpha_k\)</span>) ;
<li>
Seleciona palavras do dicionário <span class="math inline">\(1,2,...,M\)</span>, onde cada palavra do documento <span class="math inline">\(d\)</span> seleciona seu tópico <span class="math inline">\(z_{dn}\)</span>.
<li>
<span class="math inline">\(z_{dn}\)</span> é uma variável aleatória discreta com valores <span class="math inline">\(1,2,3,...,k\)</span> e probabilidades associadas <span class="math inline">\(\theta_d = (\theta_{d1}, \theta_{d2}, \theta_{d3}, ..., \theta_{dk})\)</span>;
<li>
<span class="math inline">\(z_{dn}\)</span> é o tópico de onde a palavra <span class="math inline">\(n\)</span> do documento <span class="math inline">\(d\)</span> vai ser escolhida.
<li>
<span class="math inline">\(z_{dn}\)</span> são i.i.d com distribuição <span class="math inline">\(Mult(1;\theta_d\)</span>);
<li>
Dado o tópico <span class="math inline">\(z_{dn}\)</span>, a <span class="math inline">\(n\)</span>-ésima palavra do documento <span class="math inline">\(d\)</span> é escolhida do dicionário <span class="math inline">\(1,2,...,M\)</span> com probabilidades dados por <span class="math inline">\(\beta_{z_{dn}}\)</span>, vetor associado ao seu tópico.
</ul>
<p>O vetor de parâmetros pode ser assim visualizado :</p>
<span class="math display">\[\begin{array}{cc} 
z_{11} &amp; z_{12} &amp; ... &amp; ... &amp;z_{1N}\\
z_{21} &amp; z_{22} &amp; ... &amp; ... &amp;z_{2N}\\
... &amp; ... &amp; ... &amp; ... &amp; ... &amp;\\
z_{D1} &amp; z_{D2} &amp; ... &amp; ... &amp; z_{DN} \\
\\
\theta_{11} &amp; \theta_{11} &amp; ... &amp; \theta_{1K} \\
\theta_{21} &amp; \theta_{21} &amp; ... &amp; \theta_{2K} \\
... &amp; ... &amp; ... &amp; ...\\
\theta_{D1} &amp; \theta_{D1} &amp; ... &amp; \theta_{DK} \\
\\
\beta_{11} &amp; \beta_{12} &amp; ... &amp; ... &amp;\beta_{1M}\\
\beta_{21} &amp; \beta_{22} &amp; ... &amp; ... &amp;\beta_{2M}\\
... &amp; ... &amp; ... &amp; ... &amp; ... &amp;\\
\beta_{D1} &amp; \beta_{D2} &amp; ... &amp; ... &amp;\beta_{DM} \\
\end{array}\]</span>
<div class="figure">
<img src="/home/milton/MiltonNote/Stat4Good/2016-2/Milton-LDA%20GibbsColapsado/FiguraGraphLDA.png" alt="Figura 1 : Modelo LDA" />
<p class="caption">Figura 1 : Modelo LDA</p>
</div>
<p>A partir do gráfico acima obtemos a distribuição conjunta dos dados e das variáveis, a saber :</p>
<span class="math display">\[\begin{equation}
p(\textbf{z},\textbf{w},\theta,\phi | \alpha,\beta) = p(\textbf{z} | \theta) p(\textbf{w} |\phi_z)p(\phi | \beta) p(\theta | \alpha)  
\end{equation}\]</span>
<p></font></p>
<h3>
<b> <u> 1.2 - Algoritmo <i> Gibbs Sampler LDA </i> Padrão </u> </b>
</h3>
<p><font size="5"> O algoritmo <i>Gibbs Sampler</i> padrão necessita das distribuições condicionais completas de cada variável que se deseja amostrar. A seguir apresentamos as condicionais completas das variáveis do modelo LDA calculadas a partir da distribuição conjunta apresentada no expressão anterior.</p>
<span class="math display">\[\begin{equation}
(\phi_k | \textbf{z},\textbf{w}, \theta, \phi_{-\phi_k}) \sim Dir(\beta_1 + n_{k,1},..., \beta_W + n_{k,M})
\end{equation}\]</span>
<span class="math display">\[\begin{equation}
(\theta_d | \textbf{z},\textbf{w}, \phi, \theta_{-\theta_d}) \sim Dir(\alpha_1 + n_{d,1},..., \alpha_k + n_{d,K})
\end{equation}\]</span>
<span class="math display">\[\begin{equation}
P(z_{d,n} | z_{-z_{d,n}},\textbf{w}, \phi, \theta) \propto \theta_{d,k}\phi_{k,X(d,n)}
\end{equation}\]</span>
<ul>
<li>
<span class="math inline">\(n_{k,m}\)</span> = número de vezes que o tópico <span class="math inline">\(k\)</span> e a palavra <span class="math inline">\(m\)</span> foram escolhidos.
<li>
<span class="math inline">\(n_{d,k}\)</span> = número de palavras do documento <span class="math inline">\(d\)</span> que pertencem ao tópico <span class="math inline">\(k\)</span>.
<li>
<span class="math inline">\(X\)</span> = matriz de dimensão D × N onde <span class="math inline">\(X[d, n]\)</span> é o índice da <span class="math inline">\(n\)</span>-ésima palavra no documento <span class="math inline">\(d\)</span>. Isto é, <span class="math inline">\(x[d, n] = m\)</span> significa que a <span class="math inline">\(n\)</span>-ésima palavra do documento <span class="math inline">\(d\)</span> foi a palavra <span class="math inline">\(m\)</span> do dicionário.
<li>
<p><span class="math inline">\(X[d, n]\)</span> = valores coletados nos documentos.</p>
</ul>
<p>A seguir apresentamos o altoritmo <i> Gibbs Sampler </i> Padrão:</p>
<p><b>Inicialização</b> : <span class="math inline">\(\theta^{(0)}\)</span>, <span class="math inline">\(\phi^{(0)}\)</span>, <span class="math inline">\(z^{(0)}\)</span>, <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>; sendo <span class="math inline">\(\theta\)</span> e <span class="math inline">\(\phi\)</span> <i>Dirichlet</i> simétrica</p>
<p><b>Para cada</b> <i> iteração</i> <span class="math inline">\(i\)</span> <b>faça : </b></p>
<ol>
<b>Para cada </b> <b>tópico </b> <span class="math inline">\(k\)</span> <b>calcule</b> <span class="math inline">\(n_{k,m}\)</span>
</ol>
<ol>
<b>Simule </b> <span class="math inline">\(\phi\)</span> a partir de uma <i>dirichlet</i> com parâmetros (<span class="math inline">\(n_{k,m}\)</span> + <span class="math inline">\(\beta\)</span>)
</ol>
<ol>
<b>Para cada </b> <i>documento</i> <span class="math inline">\(d\)</span> <b>faça :</b>
</ol>
<ol>
<ol>
<b>Para cada </b> <i> tópico </i> <span class="math inline">\(k\)</span> <b>calcule</b> <span class="math inline">\(n_{d,k}\)</span>
</ol>
</ol>
<ol>
<ol>
<b>Simule </b> <span class="math inline">\(\theta\)</span> a partir de uma <i>dirichlet </i> com parâmetros (<span class="math inline">\(n_{d,k}\)</span> e <span class="math inline">\(\alpha\)</span>)
</ol>
</ol>
<ol>
<ol>
<b>Para cada </b> <i>palavra</i> <span class="math inline">\(n\)</span> <b>Simule </b> <span class="math inline">\(z_{d,n}\)</span> a partir de <span class="math inline">\(\theta\)</span> e <span class="math inline">\(\phi\)</span>
</ol>
</ol>
<ol>
<b>Fim para </b>
</ol>
<p><b>Fim para </b></p>
<p></font></p>
<h3>
<b> <u> 1.3 - Algoritmo <i>Collapsed Gibbs Sampler </i> LDA </u> </b>
</h3>
<p><font size="5"></p>
<p>Antes de apresentarmos o algoritmo <i>collapsed Gibbs Sampler</i> (CGS) vamos detalhar a distribuição de probabilidade para a qual o algoritmo gera amostras.</p>
<p>O algoritmo CGS calcula a distribuição de probabildade de um tópico <span class="math inline">\(z\)</span> atribuído a uma palavra <span class="math inline">\(m\)</span>, dado todas as palavras e todos os demais tópicos atribuídos a todas as outras palavras.</p>
<p>Formalmente estamos interessados em calcular a seguinte distribuição <i>a posteriori</i>, a menos de uma constante : <span class="math display">\[p(z_i | \mathbf{z_{-i}},\textbf{w},\alpha, \beta)\]</span></p>
<p>O algoritmo CGS calcula a probabilidade acima a partir do cálculo da distribuição marginal conjunta de (<b>z</b>,<b>w</b>), através da integração da distribuição conjunta de todos os dados e variáveis, <span class="math inline">\(p(\textbf{z},\textbf{w},\theta,\phi | \alpha,\beta)\)</span>, sobre as variáveis <span class="math inline">\(\theta\)</span> e <span class="math inline">\(\phi\)</span>, ou seja :</p>
<span class="math display">\[\begin{equation} 
p(\textbf{z},\textbf{w}| \alpha,\beta) = \int \int p(\textbf{z},\textbf{w},\theta,\phi | \alpha,\beta)d\theta d\phi 
\end{equation}\]</span>
<p>Assim, pela regra de Bayes, temos que <span class="math inline">\(p(\textbf{z} | \textbf{w}, \alpha,\beta) \propto p(\textbf{z},\textbf{w} | \alpha,\beta)\)</span>, e a partir das regras de probabilidade condicional, temos :</p>
<span class="math display">\[\begin{equation} 
p(z_i | \mathbf{z_{-i}},\textbf{w},\alpha, \beta) = \frac{p(z_i , \mathbf{z_{-i}},\textbf{w} | \alpha, \beta)}{p(\mathbf{z_{-i}},\textbf{w} | \alpha, \beta)} \propto p(z_i , \mathbf{z_{-i}},\textbf{w} | \alpha, \beta) = p(\textbf{z},\textbf{w} | \alpha, \beta)
\end{equation}\]</span>
<p>onde <span class="math inline">\(\mathbf{z_{-i}}\)</span> significa todas as atribuições de tópicos efetuadas às palavras, exceto para <span class="math inline">\({z_i}\)</span>.</p>
<p>Seguindo o modelo LDA onde</p>
<p><span class="math display">\[p(\textbf{z},\textbf{w},\theta,\phi | \alpha,\beta) = p(\textbf{z} | \theta) p(\textbf{w} |\phi_z)p(\phi | \beta) p(\theta | \alpha) \]</span></p>
<p>podemos expandir a equação (1) e obter :</p>
<span class="math display">\[\begin{equation} 
p(\textbf{z},\textbf{w}| \alpha,\beta) = \int \int p(\textbf{z} | \theta) p(\textbf{w} |\phi_z)p(\phi | \beta) p(\theta | \alpha)d\theta d\phi 
\end{equation}\]</span>
<p>Agrupando os termos que têm variáveis dependentes, obtemos :</p>
<span class="math display">\[\begin{equation} 
p(\textbf{z},\textbf{w}| \alpha,\beta) = \int  p(\textbf{z} | \theta)p(\theta | \alpha)d\theta \int p(\textbf{w} |\phi_z)p(\phi | \beta)  d\phi 
\end{equation}\]</span>
<p>Observamos que as duas integrais são distribuições de probabilidade compostas de distribuições multinomiais com distribuições <i>dirichlet</i>, resultando no produto de duas distribuições .</p>
<p>Começando com o primeiro termo, temos :</p>
<p><span class="math display">\[ \int  p(\textbf{z} | \theta)p(\theta | \alpha)d\theta = \int  \prod_d \prod_i \theta_{d,z_i} \frac{1}{\beta(\mathbf{\alpha})} \prod_k \theta_{d,k}^{\alpha_k - 1}d\theta_d\]</span></p>
<span class="math display">\[\begin{equation} 
\int  p(\textbf{z} | \theta)p(\theta | \alpha)d\theta = \int \prod_d \frac{1}{B(\mathbf{\alpha})}  \prod_k \theta_{d,k}^{n_{d,k} + \alpha_k - 1}d\theta_d = \prod_d \frac{B([n_{d,k}]_{k=1}^K + \alpha)}{B(\mathbf{\alpha})} 
\end{equation}\]</span>
<p>onde <span class="math inline">\(\alpha = (\alpha_1,..., \alpha_k)\)</span>, <span class="math inline">\(B(\alpha)\)</span> é a função beta multinomial, e <span class="math inline">\(n_{d,k}\)</span> = número de vezes que palavras do documento <span class="math inline">\(d\)</span> foram atribuídas ao tópico <span class="math inline">\(k\)</span>.</p>
<p>Para o segundo termo, temos :</p>
<p><span class="math display">\[ \int p(\textbf{w} |\phi_z)p(\phi | \beta)  d\phi = \int \prod_d \prod_i \phi_{z_{d,i},w_{d,i}} \prod_k \frac{1}{B(\beta)} \prod_w \phi_{k,w}^{\beta_w - 1}d\phi_k\]</span></p>
<span class="math display">\[\begin{equation} 
\int p(\textbf{w} |\phi_z)p(\phi | \beta)  d\phi = \prod_k \frac{1}{B(\beta)} \int \prod_w \phi_{k,w}^{n_{k,w} + \beta_w - 1}d\phi_k = \prod_k \frac{B([n_{k,w}]_{w=1}^W + \beta)}{B(\mathbf{\beta})} 
\end{equation}\]</span>
<p>onde <span class="math inline">\(\beta = (\beta_1,..., \beta_k)\)</span>, e <span class="math inline">\(n_{k,w}\)</span> = número de vezes que a palavra <span class="math inline">\(w\)</span> foi atribuída ao tópico <span class="math inline">\(k\)</span>.</p>
<p>Combinando as equações (5) e (6), a distribuição conjunta <span class="math inline">\((\textbf{z}, \textbf{w})\)</span> é dada por:</p>
<span class="math display">\[\begin{equation}
p(\textbf{z},\textbf{w}| \alpha,\beta) = \prod_d \frac{B([n_{d,k}]_{k=1}^K + \alpha)}{B(\mathbf{\alpha})}  \prod_k \frac{B([n_{k,w}]_{w=1}^W + \beta)}{B(\mathbf{\beta})} 
\end{equation}\]</span>
<p>Retornando à equação (2) (mas sem os hiperparâmetros <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span> para simplificar), temos :</p>
<p><span class="math display">\[p(z_i = k | \mathbf{z_{-i}},\textbf{w}) = \frac{p(\mathbf{z},\textbf{w})}{p(\mathbf{z_{-i}},\textbf{w})} \propto \frac{B([n_{d,k}]_{k=1}^K + \alpha)}{B([n_{(d,k)}^{(-i)}]_{k=1}^K + \alpha)}  \frac{B([n_{k,w}]_{w=1}^W + \beta)}{B([n_{(k,w)}^{(-i)}]_{w=1}^W + \beta)} \]</span></p>
<p><span class="math display">\[p(z_i = k | \mathbf{z_{-i}},\textbf{w}) \propto \frac{\Gamma(n_{d,k} + \alpha_k)\Gamma(\sum_{k=1}^K n_{(d,k)}^{(-i)} + \alpha_k)}{\Gamma(n_{(d,k)}^{(-i)} + \alpha_k)\Gamma(\sum_{k=1}^K n_{d,k} + \alpha_k)}  \frac{\Gamma(n_{k,w} + \beta_w)\Gamma(\sum_{w=1}^W n_{(k,w)}^{(-i)} + \beta_w)}{\Gamma(n_{(k,w)}^{(-i)} + \beta_w) \Gamma(\sum_{w=1}^W n_{k,w} + \beta_w)} \]</span></p>
<span class="math display">\[\begin{equation}
p(z_i = k | \mathbf{z_{-i}},\textbf{w}) \propto \frac{n_{(d,k)}^{(-i)} + \alpha_k}{\sum_{k=1}^K (n_{d,k} + \alpha_k)}  \frac{n_{(k,w)}^{(-i)} + \beta_w}{\sum_{w=1}^W (n_{k,w} + \beta_w)}
\end{equation}\]</span>
<p>Uma vez apresentado a distribuição de probabilidades para a qual o algoritmo CGS irá gerar amostras e considerando que os hiperparâmetros <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span> são atribuídos um único valor (<i>dirichlet</i> simétrica), segue abaixo o algoritmo para gerar essas amostras :</p>
<p><b>Entrada :</b> palavras <b>w</b> <span class="math inline">\(\in\)</span> documentos <b>d</b></p>
<p><b>Saída :</b> tópicos <b>z</b> e contadores <span class="math inline">\(n_{d,k}\)</span>, <span class="math inline">\(n_{k,m}\)</span>, <span class="math inline">\(n_k\)</span>, e <span class="math inline">\(n_d\)</span></p>
<p><b>Início</b></p>
<ol>
Inicialize aleatoriamente <b>z</b> e incrementa contadores
</ol>
<ol>
<b>Para cada</b> <i>documento</i> <span class="math inline">\(d\)</span> <b>faça:</b>
</ol>
<ol>
<ol>
<b>Para</b> <span class="math inline">\(i = 0\)</span> até <span class="math inline">\(N\)</span> - 1 <b>faça:</b>
</ol>
</ol>
<ol>
<ol>
<ol>
<span class="math inline">\(palavra\)</span> = <span class="math inline">\(w[i]\)</span>
</ol>
</ol>
</ol>
<ol>
<ol>
<ol>
<span class="math inline">\(topico\)</span> = <span class="math inline">\(z[i]\)</span>
</ol>
</ol>
</ol>
<ol>
<ol>
<ol>
<span class="math inline">\(n_{d,topico}\)</span> -= 1
</ol>
</ol>
</ol>
<ol>
<ol>
<ol>
<span class="math inline">\(n_{topico,palavra}\)</span> -= 1
</ol>
</ol>
</ol>
<ol>
<ol>
<ol>
<span class="math inline">\(n_{topico}\)</span> -= 1
</ol>
</ol>
</ol>
<ol>
<ol>
<ol>
<b>Para</b> <span class="math inline">\(k = 0\)</span> até <span class="math inline">\(K\)</span> - 1 <b>faça:</b>
</ol>
</ol>
</ol>
<ol>
<ol>
<ol>
<ol>
<span class="math inline">\(p(z = k | .)\)</span> = <span class="math inline">\(\frac{n_{d,k} + \alpha_k}{n_d + K \alpha} \frac{n_{k,w} + \beta_w}{n_k + W \beta}\)</span>
</ol>
</ol>
</ol>
</ol>
<ol>
<ol>
<ol>
<b>Fim Para</b>
</ol>
</ol>
</ol>
<ol>
<ol>
<ol>
<l><span class="math inline">\(topico\)</span> = <l>sample de <span class="math inline">\(p(z | .)\)</span>
</ol>
</ol>
</ol>
<ol>
<ol>
<ol>
<span class="math inline">\(z[i]\)</span> = <span class="math inline">\(topico\)</span>
</ol>
</ol>
</ol>
<ol>
<ol>
<ol>
<span class="math inline">\(n_{d,topico}\)</span> += 1
</ol>
</ol>
</ol>
<ol>
<ol>
<ol>
<span class="math inline">\(n_{topico,palavra}\)</span> += 1
</ol>
</ol>
</ol>
<ol>
<ol>
<ol>
<span class="math inline">\(n_{topico}\)</span> += 1
</ol>
</ol>
</ol>
<ol>
<ol>
<b>Fim Para</b>
</ol>
</ol>
<ol>
<b>Fim Para</b>
</ol>
<ol>
<b>retorne z</b>,<span class="math inline">\(n_{d,k}, n_{k,w}, n_k, n_d\)</span>
</ol>
<p><b>Fim</b></p>
<p>Como as distribuições a posteriori de <span class="math inline">\(\theta\)</span> e <span class="math inline">\(\phi\)</span> são distribuições <i>dirichlet</i></p>
<p><span class="math display">\[(\theta_d | .) \sim Dir(n_{d,1} + \alpha_1, ..., n_{d,k} + \alpha_k)\]</span> <span class="math display">\[(\phi_w | .) \sim Dir(n_{k,1} + \beta_1, ..., n_{k,w} + \beta_k)\]</span></p>
<p>para termos uma estimativa dessas variáveis, basta calcularmos o valor esperado de cada um conforme abaixo :</p>
<p><span class="math display">\[\hat{\phi}_{k,w} = \frac{n_{k,w} + \beta_k}{n_k + W \beta } \]</span> <span class="math display">\[\hat{\theta}_{d,k} = \frac{n_{d,k} +\alpha_k}{n_d + K \alpha } \]</span></p>
<p></font></p>
<h2>
<b> 2 - Bases de dados de documentos </b>
</h2>
<p><font size="5"></p>
<p>A seguinte coleção de documentos foi utilizada :</p>
<ul>
<li>
<i> Journal of Statistical Software </i> (JSS) até 08/05/2010 utilizada no artigo <i> topicmodels: An R Package for Fitting Topic Models </i>.
</li>
</ul>
<p></font></p>
<h2>
<b> 3 - Comparação dos resultados </b>
</h2>
<p><font size="5"></p>
<p>Nesse item apresentaremos a comparação entre os resultados das implementações :</p>
<ul>
<li>
Implementação própria do <i>Gibbs Sampler</i> Padrão;
</li>
<li>
Implementação própria do <i>Collapsed Gibbs Sampler</i>
</li>
</ul>
<p></font></p>

                        </div>
                        
                        

                    </div>
                    

                    

                    

                    <div class="col-md-3">

                        

                        







<div class="panel panel-default sidebar-menu">

    <div class="panel-heading">
      <h3 class="panel-title"></h3>
    </div>

    <div class="panel-body">
        <ul class="nav nav-pills nav-stacked">
            
            <li><a href="/categories/modelos">modelos (3)</a>
            </li>
            
            <li><a href="/categories/tutoriais-r">tutoriais-r (6)</a>
            </li>
            
        </ul>
    </div>
</div>








<div class="panel sidebar-menu">
    <div class="panel-heading">
      <h3 class="panel-title"></h3>
    </div>

    <div class="panel-body">
        <ul class="tag-cloud">
            
            <li><a href="/tags/dplyr"><i class="fa fa-tags"></i> dplyr</a>
            </li>
            
            <li><a href="/tags/equa%c3%a7%c3%b5es-estruturais"><i class="fa fa-tags"></i> equações-estruturais</a>
            </li>
            
            <li><a href="/tags/express%c3%b5es-regulares"><i class="fa fa-tags"></i> expressões-regulares</a>
            </li>
            
            <li><a href="/tags/importando-dados"><i class="fa fa-tags"></i> importando-dados</a>
            </li>
            
            <li><a href="/tags/manipula%c3%a7%c3%a3o-de-dados"><i class="fa fa-tags"></i> manipulação-de-dados</a>
            </li>
            
            <li><a href="/tags/modelagem"><i class="fa fa-tags"></i> modelagem</a>
            </li>
            
            <li><a href="/tags/modelos"><i class="fa fa-tags"></i> modelos</a>
            </li>
            
            <li><a href="/tags/paraleliza%c3%a7%c3%a3o"><i class="fa fa-tags"></i> paralelização</a>
            </li>
            
            <li><a href="/tags/r"><i class="fa fa-tags"></i> r</a>
            </li>
            
            <li><a href="/tags/r-markdown"><i class="fa fa-tags"></i> r-markdown</a>
            </li>
            
            <li><a href="/tags/relat%c3%b3rio"><i class="fa fa-tags"></i> relatório</a>
            </li>
            
            <li><a href="/tags/visualiza%c3%a7%c3%a3o"><i class="fa fa-tags"></i> visualização</a>
            </li>
            
        </ul>
    </div>
</div>






                        

                    </div>
                    

                    

                </div>
                

            </div>
            
        </div>
        

        
<div id="top">
  <div class="container">
    <div class="row">
      <div class="col-xs-5">
        <p class="hidden-sm hidden-xs">Contact us stats4good@gmail.com</p>
      <p class="hidden-md hidden-lg"><a href="#" data-animate-hover="pulse"><i class="fa fa-phone"></i></a>
      <a href="#" data-animate-hover="pulse"><i class="fa fa-envelope"></i></a>
      </p>
      
      </div>
      <div class="col-xs-7">
        <div class="social">
          
          <a href="https://github.com/stats4good" target="_blank" style="opacity: 1;"><i class='fa fa-2x fa-github'></i></a>
          
          <a href="mailto:stats4good@gmail.com" target="_blank" style="opacity: 1;"><i class='fa fa-2x fa-envelope'></i></a>
          
        </div>
      </div>
    </div>
  </div>
</div>



    </div>
    

    
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-107006863-1', 'auto');
ga('send', 'pageview');
</script>

<script src="//code.jquery.com/jquery-3.1.1.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/waypoints/4.0.1/jquery.waypoints.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/Counter-Up/1.0/jquery.counterup.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/jquery-parallax/1.1.3/jquery-parallax.js"></script>

<script src="//maps.googleapis.com/maps/api/js?v=3.exp"></script>

<script src="/js/hpneo.gmaps.js"></script>
<script src="/js/gmaps.init.js"></script>
<script src="/js/front.js"></script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  });
</script>
<script type="text/javascript"
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<script src="/js/owl.carousel.min.js"></script>


  </body>
</html>
